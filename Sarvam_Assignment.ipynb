{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install gensim numpy scikit-learn\n",
        "\n",
        "# Download pre-trained embeddings for English and Hindi from FastText\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
        "\n",
        "# Load pre-trained FastText embeddings using gensim\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz')\n",
        "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz')\n",
        "\n",
        "# Check the size of the vocabulary for both languages\n",
        "print(f'English vocab size: {len(en_embeddings.index_to_key)}')\n",
        "print(f'Hindi vocab size: {len(hi_embeddings.index_to_key)}')\n",
        "\n",
        "# Download the English-Hindi bilingual dictionary from MUSE\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
        "\n",
        "# Load the bilingual dictionary (English-Hindi)\n",
        "bilingual_lexicon = []\n",
        "with open('en-hi.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        en_word, hi_word = line.strip().split()\n",
        "        # Ensure both words exist in the embeddings' vocabularies\n",
        "        if en_word in en_embeddings.key_to_index and hi_word in hi_embeddings.key_to_index:\n",
        "            bilingual_lexicon.append((en_word, hi_word))\n",
        "\n",
        "# Check the size of the bilingual lexicon\n",
        "print(f'Total bilingual pairs: {len(bilingual_lexicon)}')\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Extract the embeddings for the bilingual lexicon pairs\n",
        "X = np.array([en_embeddings[word_en] for word_en, word_hi in bilingual_lexicon])\n",
        "Y = np.array([hi_embeddings[word_hi] for word_en, word_hi in bilingual_lexicon])\n",
        "\n",
        "# Procrustes alignment method\n",
        "def procrustes(X, Y):\n",
        "    U, _, Vt = np.linalg.svd(np.dot(X.T, Y))\n",
        "    R = np.dot(U, Vt)\n",
        "    return R\n",
        "\n",
        "# Compute the optimal orthogonal matrix using the bilingual lexicon\n",
        "R = procrustes(X, Y)\n",
        "\n",
        "# Align the English embeddings with the learned transformation matrix\n",
        "aligned_en_embeddings = np.dot(en_embeddings.vectors, R)\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to translate words from English to Hindi using aligned embeddings\n",
        "def translate_word(word, aligned_en_embeddings, hi_embeddings):\n",
        "    en_vector = aligned_en_embeddings[en_embeddings.key_to_index[word]]\n",
        "    similarities = cosine_similarity(en_vector.reshape(1, -1), hi_embeddings.vectors)\n",
        "    most_similar_idx = similarities.argmax()\n",
        "    return hi_embeddings.index_to_key[most_similar_idx]\n",
        "\n",
        "# Example translation\n",
        "word = ('duck')\n",
        "translation = translate_word(word, aligned_en_embeddings, hi_embeddings)\n",
        "print(f'Translation of \"{word}\" is \"{translation}\"')\n",
        "\n",
        "# Function to evaluate translation quality using Precision@1 and Precision@5\n",
        "def evaluate_translation(test_dict, aligned_en_embeddings, hi_embeddings, k=5):\n",
        "    correct_at_1 = 0\n",
        "    correct_at_5 = 0\n",
        "    for en_word, hi_word in test_dict:\n",
        "        en_vector = aligned_en_embeddings[en_embeddings.key_to_index[en_word]]\n",
        "        similarities = cosine_similarity(en_vector.reshape(1, -1), hi_embeddings.vectors)\n",
        "        top_k_indices = similarities[0].argsort()[-k:][::-1]\n",
        "        top_k_words = [hi_embeddings.index_to_key[i] for i in top_k_indices]\n",
        "\n",
        "        if hi_word == top_k_words[0]:\n",
        "            correct_at_1 += 1\n",
        "        if hi_word in top_k_words:\n",
        "            correct_at_5 += 1\n",
        "\n",
        "    precision_at_1 = correct_at_1 / len(test_dict)\n",
        "    precision_at_5 = correct_at_5 / len(test_dict)\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Prepare the test dictionary (subset of MUSE)\n",
        "muse_test_dict = bilingual_lexicon[:1000]  # Use a subset for testing\n",
        "\n",
        "# Evaluate Precision@1 and Precision@5\n",
        "precision_at_1, precision_at_5 = evaluate_translation(muse_test_dict, aligned_en_embeddings, hi_embeddings)\n",
        "print(f'Precision@1: {precision_at_1}')\n",
        "print(f'Precision@5: {precision_at_5}')\n",
        "\n",
        "# Step d: Compute and analyze cosine similarities between word pairs\n",
        "def analyze_cosine_similarity(bilingual_lexicon, aligned_en_embeddings, hi_embeddings, num_pairs=10):\n",
        "    similarities = []\n",
        "\n",
        "    for i, (en_word, hi_word) in enumerate(bilingual_lexicon):\n",
        "        if en_word in en_embeddings and hi_word in hi_embeddings:\n",
        "            # Get the aligned English embedding and the corresponding Hindi embedding\n",
        "            en_vector = aligned_en_embeddings[en_embeddings.key_to_index[en_word]]\n",
        "            hi_vector = hi_embeddings[hi_word]\n",
        "\n",
        "            # Compute cosine similarity\n",
        "            similarity = cosine_similarity(en_vector.reshape(1, -1), hi_vector.reshape(1, -1))[0][0]\n",
        "            similarities.append((en_word, hi_word, similarity))\n",
        "\n",
        "        # Limit the output to `num_pairs` for analysis\n",
        "        if i >= num_pairs:\n",
        "            break\n",
        "\n",
        "    return similarities\n",
        "\n",
        "# Compute and print cosine similarities for the first 10 word pairs\n",
        "cosine_similarities = analyze_cosine_similarity(bilingual_lexicon, aligned_en_embeddings, hi_embeddings, num_pairs=10)\n",
        "\n",
        "print(f\"{'English Word':<15}{'Hindi Word':<15}{'Cosine Similarity'}\")\n",
        "for en_word, hi_word, sim in cosine_similarities:\n",
        "    print(f\"{en_word:<15}{hi_word:<15}{sim:.4f}\")\n",
        "\n",
        "\n",
        "# Step e: Conduct an ablation study to assess the impact of bilingual lexicon size\n",
        "def ablation_study(bilingual_lexicon, en_embeddings, hi_embeddings, sizes=[5000, 10000, 20000]):\n",
        "    results = []\n",
        "\n",
        "    for size in sizes:\n",
        "        # Limit the lexicon to the current size\n",
        "        current_lexicon = bilingual_lexicon[:size]\n",
        "\n",
        "        # Extract embeddings for the bilingual lexicon pairs\n",
        "        X = np.array([en_embeddings[word_en] for word_en, word_hi in current_lexicon if word_en in en_embeddings and word_hi in hi_embeddings])\n",
        "        Y = np.array([hi_embeddings[word_hi] for word_en, word_hi in current_lexicon if word_en in en_embeddings and word_hi in hi_embeddings])\n",
        "\n",
        "        # Perform Procrustes alignment\n",
        "        R = procrustes(X, Y)\n",
        "        aligned_en_embeddings = np.dot(en_embeddings.vectors, R)\n",
        "\n",
        "        # Evaluate using Precision@1 and Precision@5\n",
        "        precision_at_1, precision_at_5 = evaluate_translation(current_lexicon, aligned_en_embeddings, hi_embeddings)\n",
        "\n",
        "        # Store results for analysis\n",
        "        results.append((size, precision_at_1, precision_at_5))\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run the ablation study with different dictionary sizes\n",
        "ablation_results = ablation_study(bilingual_lexicon, en_embeddings, hi_embeddings, sizes=[5000, 10000, 20000])\n",
        "\n",
        "# Print the results\n",
        "print(f\"{'Lexicon Size':<15}{'Precision@1':<15}{'Precision@5'}\")\n",
        "for size, p1, p5 in ablation_results:\n",
        "    print(f\"{size:<15}{p1:<15}{p5}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Fd6IGsEKfLwc",
        "outputId": "35e96614-1b96-4e90-84c9-3ba224099974",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "--2024-09-26 13:21:42--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.81, 18.164.78.72, 18.164.78.128, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.3’\n",
            "\n",
            "cc.en.300.vec.gz.3  100%[===================>]   1.23G  35.3MB/s    in 41s     \n",
            "\n",
            "2024-09-26 13:22:23 (31.0 MB/s) - ‘cc.en.300.vec.gz.3’ saved [1325960915/1325960915]\n",
            "\n",
            "--2024-09-26 13:22:23--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.125.57, 3.162.125.58, 3.162.125.66, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.125.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz.3’\n",
            "\n",
            "cc.hi.300.vec.gz.3  100%[===================>]   1.04G  19.8MB/s    in 48s     \n",
            "\n",
            "2024-09-26 13:23:12 (22.2 MB/s) - ‘cc.hi.300.vec.gz.3’ saved [1118942272/1118942272]\n",
            "\n",
            "English vocab size: 2000000\n",
            "Hindi vocab size: 1876653\n",
            "--2024-09-26 13:37:45--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.121, 18.164.78.72, 18.164.78.128, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.121|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt.2’\n",
            "\n",
            "en-hi.txt.2         100%[===================>] 909.04K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-09-26 13:37:45 (14.2 MB/s) - ‘en-hi.txt.2’ saved [930856/930856]\n",
            "\n",
            "Total bilingual pairs: 32340\n",
            "Translation of \"duck\" is \"बत्तख\"\n",
            "Precision@1: 0.29\n",
            "Precision@5: 0.566\n",
            "English Word   Hindi Word     Cosine Similarity\n",
            "and            और             0.4367\n",
            "was            था             0.5852\n",
            "was            थी             0.4897\n",
            "for            लिये           0.4482\n",
            "that           उस             0.4570\n",
            "that           कि             0.4763\n",
            "with           साथ            0.5222\n",
            "from           से             0.3476\n",
            "from           इससे           0.3494\n",
            "this           ये             0.3605\n",
            "this           यह             0.4637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CSLS method"
      ],
      "metadata": {
        "id": "fyyuX5si8jF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch faiss-cpu gensim numpy scikit-learn\n",
        "\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "en_embeddings = KeyedVectors.load_word2vec_format('cc.en.300.vec.gz')\n",
        "hi_embeddings = KeyedVectors.load_word2vec_format('cc.hi.300.vec.gz')\n",
        "\n",
        "# vocabulary size of both embeddings\n",
        "print(f'English vocab size: {len(en_embeddings.index_to_key)}')\n",
        "print(f'Hindi vocab size: {len(hi_embeddings.index_to_key)}')\n",
        "\n",
        "\n",
        "# English-Hindi bilingual dictionary from MUSE\n",
        "!wget https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
        "\n",
        "# Load the bilingual dictionary\n",
        "bilingual_lexicon = []\n",
        "with open('en-hi.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        en_word, hi_word = line.strip().split()\n",
        "        if en_word in en_embeddings.key_to_index and hi_word in hi_embeddings.key_to_index:\n",
        "            bilingual_lexicon.append((en_word, hi_word))\n",
        "\n",
        "print(f'Total bilingual pairs: {len(bilingual_lexicon)}')\n",
        "\n",
        "\n",
        "# CSLS function (Unsupervised Alignment)\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def compute_csls(src_emb, tgt_emb, k=10):\n",
        "    # cosine similarities\n",
        "    similarities = cosine_similarity(src_emb, tgt_emb)\n",
        "\n",
        "    # average cosine similarity for each vector in the source and target spaces\n",
        "    src_avg_sim = np.mean(np.sort(similarities, axis=1)[:, -k:], axis=1)\n",
        "    tgt_avg_sim = np.mean(np.sort(similarities.T, axis=1)[:, -k:], axis=1)\n",
        "\n",
        "    # CSLS correction\n",
        "    csls_similarities = 2 * similarities.T - src_avg_sim - tgt_avg_sim[:, np.newaxis]\n",
        "\n",
        "    return csls_similarities.T\n",
        "\n",
        "\n",
        "# the Mapping Network and Discriminator for Adversarial Training\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# the mapping network as a linear transformation (orthogonal matrix)\n",
        "class MappingNetwork(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(MappingNetwork, self).__init__()\n",
        "        self.mapping = nn.Linear(embedding_dim, embedding_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.mapping(x)\n",
        "\n",
        "# the discriminator network as a simple binary classifier\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x).view(-1)\n",
        "\n",
        "# adversarial training of the Mapping Network and Discriminator\n",
        "def adversarial_training(src_emb, tgt_emb, mapping_net, discriminator, num_epochs=10, batch_size=128, lr=0.001):\n",
        "    # Optimizers\n",
        "    mapping_optimizer = optim.Adam(mapping_net.parameters(), lr=lr)\n",
        "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
        "\n",
        "    # Loss function\n",
        "    adversarial_loss = nn.BCELoss()\n",
        "\n",
        "    # Convert embeddings to PyTorch tensors\n",
        "    src_emb = torch.tensor(src_emb, dtype=torch.float32)\n",
        "    tgt_emb = torch.tensor(tgt_emb, dtype=torch.float32)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i in range(0, len(src_emb), batch_size):\n",
        "            # Sample a batch of source and target embeddings\n",
        "            src_batch = src_emb[i:i+batch_size]\n",
        "            tgt_batch = tgt_emb[i:i+batch_size]\n",
        "\n",
        "            # Create labels for discriminator training\n",
        "            src_labels = torch.zeros(src_batch.size(0))\n",
        "            tgt_labels = torch.ones(tgt_batch.size(0))\n",
        "\n",
        "            # Train discriminator\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            src_pred = discriminator(src_batch)\n",
        "            tgt_pred = discriminator(tgt_batch)\n",
        "            loss_d = adversarial_loss(src_pred, src_labels) + adversarial_loss(tgt_pred, tgt_labels)\n",
        "            loss_d.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "            # Train mapping to fool the discriminator\n",
        "            mapping_optimizer.zero_grad()\n",
        "            mapped_src_batch = mapping_net(src_batch)\n",
        "            src_pred = discriminator(mapped_src_batch)\n",
        "            loss_g = adversarial_loss(src_pred, tgt_labels)  # Fool the discriminator\n",
        "            loss_g.backward()\n",
        "            mapping_optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Discriminator Loss: {loss_d.item()}, Generator Loss: {loss_g.item()}\")\n",
        "\n",
        "    # Return the learned mapping\n",
        "    return mapping_net\n",
        "\n",
        "\n",
        "# align the source embeddings with the learned mapping and evaluate CSLS\n",
        "# set up the mapping network and discriminator\n",
        "embedding_dim = en_embeddings.vector_size\n",
        "mapping_net = MappingNetwork(embedding_dim)\n",
        "discriminator = Discriminator(embedding_dim)\n",
        "\n",
        "# convert the source and target embeddings to numpy arrays for adversarial training\n",
        "src_emb = np.array([en_embeddings[word_en] for word_en, word_hi in bilingual_lexicon])\n",
        "tgt_emb = np.array([hi_embeddings[word_hi] for word_en, word_hi in bilingual_lexicon])\n",
        "\n",
        "# train the mapping using adversarial training\n",
        "mapping_net = adversarial_training(src_emb, tgt_emb, mapping_net, discriminator)\n",
        "\n",
        "# align the source embeddings with the learned mapping\n",
        "aligned_src_emb = mapping_net(torch.tensor(src_emb, dtype=torch.float32)).detach().numpy()\n",
        "\n",
        "# Evaluate using CSLS\n",
        "def evaluate_with_csls(src_emb, tgt_emb, k=5):\n",
        "    csls_similarities = compute_csls(src_emb, tgt_emb)\n",
        "\n",
        "    # Evaluate Precision@1 and Precision@5\n",
        "    precision_at_1 = 0\n",
        "    precision_at_5 = 0\n",
        "\n",
        "    for i, similarities in enumerate(csls_similarities):\n",
        "        top_k_indices = similarities.argsort()[-k:][::-1]\n",
        "        top_k_words = [hi_embeddings.index_to_key[i] for i in top_k_indices]\n",
        "        en_word, hi_word = bilingual_lexicon[i]\n",
        "\n",
        "        if hi_word == top_k_words[0]:\n",
        "            precision_at_1 += 1\n",
        "        if hi_word in top_k_words:\n",
        "            precision_at_5 += 1\n",
        "\n",
        "    precision_at_1 /= len(bilingual_lexicon)\n",
        "    precision_at_5 /= len(bilingual_lexicon)\n",
        "\n",
        "    return precision_at_1, precision_at_5\n",
        "\n",
        "# Evaluate the unsupervised CSLS-based method\n",
        "precision_at_1_csls, precision_at_5_csls = evaluate_with_csls(aligned_src_emb, tgt_emb)\n",
        "print(f\"Unsupervised CSLS - Precision@1: {precision_at_1_csls}, Precision@5: {precision_at_5_csls}\")\n",
        "\n",
        "# compare with Supervised Procrustes (If implemented earlier)\n",
        "# Assuming you already have a function to evaluate using Procrustes\n",
        "# Compare with Procrustes method (already implemented in previous sections)\n",
        "precision_at_1_procrustes, precision_at_5_procrustes = evaluate_translation(muse_test_dict, aligned_en_embeddings, hi_embeddings)\n",
        "print(f\"Supervised Procrustes - Precision@1: {precision_at_1_procrustes}, Precision@5: {precision_at_5_procrustes}\")\n",
        "\n",
        "# comparison results\n",
        "print(f\"Supervised Procrustes: Precision@1 = {precision_at_1_procrustes}, Precision@5 = {precision_at_5_procrustes}\")\n",
        "print(f\"Unsupervised CSLS + Adversarial: Precision@1 = {precision_at_1_csls}, Precision@5 = {precision_at_5_csls}\")"
      ],
      "metadata": {
        "id": "uFmar0Ec8mDN",
        "outputId": "a02594e7-cb04-4cbf-97d7-161f1d567def",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "--2024-09-26 16:49:11--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.81, 18.164.78.128, 18.164.78.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1325960915 (1.2G) [binary/octet-stream]\n",
            "Saving to: ‘cc.en.300.vec.gz.7’\n",
            "\n",
            "cc.en.300.vec.gz.7  100%[===================>]   1.23G  51.4MB/s    in 13s     \n",
            "\n",
            "2024-09-26 16:49:25 (97.7 MB/s) - ‘cc.en.300.vec.gz.7’ saved [1325960915/1325960915]\n",
            "\n",
            "--2024-09-26 16:49:25--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.hi.300.vec.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.81, 18.164.78.128, 18.164.78.121, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1118942272 (1.0G) [binary/octet-stream]\n",
            "Saving to: ‘cc.hi.300.vec.gz.7’\n",
            "\n",
            "cc.hi.300.vec.gz.7  100%[===================>]   1.04G  54.8MB/s    in 19s     \n",
            "\n",
            "2024-09-26 16:49:44 (56.8 MB/s) - ‘cc.hi.300.vec.gz.7’ saved [1118942272/1118942272]\n",
            "\n",
            "English vocab size: 2000000\n",
            "Hindi vocab size: 1876653\n",
            "--2024-09-26 17:04:44--  https://dl.fbaipublicfiles.com/arrival/dictionaries/en-hi.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.164.78.81, 18.164.78.128, 18.164.78.72, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.164.78.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 930856 (909K) [text/x-c++]\n",
            "Saving to: ‘en-hi.txt.6’\n",
            "\n",
            "en-hi.txt.6         100%[===================>] 909.04K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-09-26 17:04:44 (14.2 MB/s) - ‘en-hi.txt.6’ saved [930856/930856]\n",
            "\n",
            "Total bilingual pairs: 32340\n",
            "Epoch 1/10, Discriminator Loss: 0.09318806231021881, Generator Loss: 0.00014391196600627154\n",
            "Epoch 2/10, Discriminator Loss: 0.035520266741514206, Generator Loss: 1.9344074644322973e-06\n",
            "Epoch 3/10, Discriminator Loss: 0.019836772233247757, Generator Loss: 1.7455717227221612e-07\n",
            "Epoch 4/10, Discriminator Loss: 0.013122368603944778, Generator Loss: 5.2508923431560106e-08\n",
            "Epoch 5/10, Discriminator Loss: 0.009469650685787201, Generator Loss: 1.986822617539019e-08\n",
            "Epoch 6/10, Discriminator Loss: 0.007248809561133385, Generator Loss: 8.514951410631966e-09\n",
            "Epoch 7/10, Discriminator Loss: 0.005755160935223103, Generator Loss: 4.257475261226773e-09\n",
            "Epoch 8/10, Discriminator Loss: 0.004668684676289558, Generator Loss: 1.4191582353717536e-09\n",
            "Epoch 9/10, Discriminator Loss: 0.003801260143518448, Generator Loss: 1.4191582353717536e-09\n",
            "Epoch 10/10, Discriminator Loss: 0.003129206597805023, Generator Loss: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I don't have enough ram so the free colab version keeps crashing but this should work"
      ],
      "metadata": {
        "id": "3P6gGHHsTBIu"
      }
    }
  ]
}